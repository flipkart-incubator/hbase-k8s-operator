{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Hbase \u00b6 HBase is an open-source non-relational distributed database modeled after Google's Bigtable and written in Java. It is developed as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System) or Alluxio, providing Bigtable-like capabilities for Hadoop. That is, it provides a fault-tolerant way of storing large quantities of sparse data. Operator Details \u00b6 This operator is designed to be Namespace scoped. Single kubernetes cluster can run multiple instances of this operator in separate namespaces listening for multiple other namespaces This operator generic enough to be able to run wide range of hbase versions. Tested across several 2.x versions Multi tenant hbase cluster which can span across multiple namespaces, where tenants can be from different teams owning their own infra and maintenance. A helm chart wrapper is present along with operator aiming to standardise deployments and avoid common pitfalls such as writing complex probes, startup and shutdown scripts, etc Generic enough to extend it with any customisations such as metric sidecars, istio sidecars, annotations, init containers etc Supports for rack awareness where fault domain can be fed in from multiple options such as file in a pod, env variable, etc and state is stored in zookeeper as a central store for building rack topology Repository Components \u00b6 Hbase Operator \u00b6 Hbase k8s operatis is a custom kubernetes controller that uses custom resource to manage hbase applications and their components. There are 3 custom resources(CR) defined here. Custom Resources \u00b6 HbaseCluster \u00b6 HbaseCluster CRD's spins up a cluster with following components in an ordered fashion Zookeeper Quorum JournalNode Quorum HA Namenodes Cluster of Datanodes + Regionservers (single pod to enable short circuiting) Hbase Masters HbaseTenant \u00b6 HbaseTenant CRD is capable of bringing up a group of datandes along with regionservers to form a rsgroup which can be grouped under different namespace. Cluster of Datanodes + Regionservers (single pod) HbaseStandalone \u00b6 HbaseStandalone CRD is capable of bringup a single pod hbase primarily used for testing purposes. Helm Chart \u00b6 Helm chart bundles the packaging aspects of hbase resource manifest in a simplified manner and can be used as dependency in your helm deployments. Following are covered under helm chart Entrypoint scripts for components such as zookeeper , journalnode , namenode , hmaster , datanode , regionserver . Where these entrypoints does necessary bootstrapping and trap SIGTERM to gracefully terminate application. InitContainers for components such as Ensure dns is resolvable well before statefulsets start Rackawareness support with optional fault domain publisher to zookeeper Namenode refresher on datanode start SideCar Containers for components such as RackUtils for constructing rack topology MTL(Monitoring, Telemetry, Logs) publisher sidecars","title":"Hbase Operator"},{"location":"#about-hbase","text":"HBase is an open-source non-relational distributed database modeled after Google's Bigtable and written in Java. It is developed as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System) or Alluxio, providing Bigtable-like capabilities for Hadoop. That is, it provides a fault-tolerant way of storing large quantities of sparse data.","title":"About Hbase"},{"location":"#operator-details","text":"This operator is designed to be Namespace scoped. Single kubernetes cluster can run multiple instances of this operator in separate namespaces listening for multiple other namespaces This operator generic enough to be able to run wide range of hbase versions. Tested across several 2.x versions Multi tenant hbase cluster which can span across multiple namespaces, where tenants can be from different teams owning their own infra and maintenance. A helm chart wrapper is present along with operator aiming to standardise deployments and avoid common pitfalls such as writing complex probes, startup and shutdown scripts, etc Generic enough to extend it with any customisations such as metric sidecars, istio sidecars, annotations, init containers etc Supports for rack awareness where fault domain can be fed in from multiple options such as file in a pod, env variable, etc and state is stored in zookeeper as a central store for building rack topology","title":"Operator Details"},{"location":"#repository-components","text":"","title":"Repository Components"},{"location":"#hbase-operator","text":"Hbase k8s operatis is a custom kubernetes controller that uses custom resource to manage hbase applications and their components. There are 3 custom resources(CR) defined here.","title":"Hbase Operator"},{"location":"#custom-resources","text":"","title":"Custom Resources"},{"location":"#hbasecluster","text":"HbaseCluster CRD's spins up a cluster with following components in an ordered fashion Zookeeper Quorum JournalNode Quorum HA Namenodes Cluster of Datanodes + Regionservers (single pod to enable short circuiting) Hbase Masters","title":"HbaseCluster"},{"location":"#hbasetenant","text":"HbaseTenant CRD is capable of bringing up a group of datandes along with regionservers to form a rsgroup which can be grouped under different namespace. Cluster of Datanodes + Regionservers (single pod)","title":"HbaseTenant"},{"location":"#hbasestandalone","text":"HbaseStandalone CRD is capable of bringup a single pod hbase primarily used for testing purposes.","title":"HbaseStandalone"},{"location":"#helm-chart","text":"Helm chart bundles the packaging aspects of hbase resource manifest in a simplified manner and can be used as dependency in your helm deployments. Following are covered under helm chart Entrypoint scripts for components such as zookeeper , journalnode , namenode , hmaster , datanode , regionserver . Where these entrypoints does necessary bootstrapping and trap SIGTERM to gracefully terminate application. InitContainers for components such as Ensure dns is resolvable well before statefulsets start Rackawareness support with optional fault domain publisher to zookeeper Namenode refresher on datanode start SideCar Containers for components such as RackUtils for constructing rack topology MTL(Monitoring, Telemetry, Logs) publisher sidecars","title":"Helm Chart"},{"location":"faq/","text":"Are there any caveats to know about this packaging Namenode caches journalnode ip address and doesn't change when address changes. You might want to patch your hdfs deployment with this patch HADOOP-17068 Minimal command line tools required for this package to work correctly nslookup netcat curl Minimal configuration for the package to work correctly Zookeeper should have zookeeper.4lw.commands enabled with bare minimum stat command Tested versions for various components Hadoop: 3.1.x Hbase: 2.1.x - 2.4.x Zookeeper: 3.4.10+ Can I change service ports for components such as zookeeper, journalnode, etc. If not what are the defaults At present, ports are hard coded with packaging, will change this in the future. Zookeeper: 2181 Journalnode: 8485 Hmaster: 16000, 16010 RegionServer: 16020, 16030 Datanode: 9866 Namenode: 8020, 50070 Zkfc: 8019 Which shell is recommend bundled with hbase docker image All tests are done with bash shell and hence can't guarantee working with any other shell.","title":"FAQ"},{"location":"setup/hbase/","text":"change directory to /examples under parent directory of this repository Build Docker Image for Hbase \u00b6 Maven POM \u00b6 pom.xml is written to bundle all of the dependencies to package in docker. Such has downloading hadoop and hbase binaries from either public repos or private repos of organisation. Optionally you can customise the package to suit your needs. Some Examples are: optional libraries for change propogation, modified hbase base libraries, repair libraries, etc. Additionally you can have modified repositories , distributionManagement , pluginRepositories so as to download dependencies from private repositories Dockerfile \u00b6 Dockerfile package hbase and hadoop binaries, which can be downloaded from public mirrors or private mirrors. User, Group, Directories required are created and given sufficient permissions for hbase to run. Optionally can modify or add additional libraries from hbase or hadoop packages Optionally utilities can be installed required such as dnsutils, netcat, etc Base image should be kept smaller, builder image can include jdk image Build docker image and publish to a repository. docker build . --network host -t hbase:2.4.2 && docker push hbase:2.4.2 Hbase Standalone \u00b6 Package and Deploy Hbase Standalone \u00b6 Helm Chart \u00b6 A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasestandalone-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasestandalone-chart/charts/ Open examples/hbasestandalone-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are image: Docker image of hbase we built in previous section Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasestandalone-chart hbasestandalone-chart/ -n hbase_standalone Hbase Cluster \u00b6 Package and Deploy Hbase Cluster \u00b6 Helm Chart \u00b6 Changing namespace names would mean configuration having host names should also be changed such as zookeeper, namenode etc A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasecluster-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasecluster-chart/charts/ Open examples/hbasecluster-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are isBootstrap: Enable this flag first time you run this cluster. Which performs hdfs format , required at the time of cluster setup. Once cluster started, you can disable and upgrade the cluster again. image: Docker image of hbase we built in previous section annotations: In this examples, we have used to demonstrate MTL (Monitoring, Telemetry and Logging) Volume claims for your k8s can be fetched using kubectl get storageclass . Which can be used to replace storageClass probeDelay : This will affect both liveness and readiness alike Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasecluster-chart hbasecluster-chart/ -n hbase_cluster Hbase Tenant \u00b6 Operator Side \u00b6 Add additional namespaces to watch for. Here specific namespace to be onboarded for a particular tenant vim operator/config/custom/config/hbase-operator-config.yaml Create configmap with command kubectl apply -f operator/config/custom/config/hbase-operator-config.yaml -n hbase_operator Deploy operator Tenant Side \u00b6 Create Rolebinding under namespace which is hosting either hbasetenant or hbasecluster such as follows. Where hbase_tenant is the namespace on which you would deploy your resources ./testbin/bin/kubectl apply -f config/rbac/role_binding.yaml -n hbase_tenant Package and Deploy Hbase Tenant \u00b6 Helm Chart \u00b6 Changing namespace names would mean configuration having host names should also be changed such as zookeeper, namenode etc A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasetenant-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasetenant-chart/charts/ Open examples/hbasetenant-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are image: Docker image of hbase we built in previous section annotations: In this examples, we have used to demonstrate MTL (Monitoring, Telemetry and Logging) Volume claims for your k8s can be fetched using kubectl get storageclass . Which can be used to replace storageClass probeDelay : This will affect both liveness and readiness alike Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasetenant-chart hbasetenant-chart/ -n hbase_tenant","title":"Deploy Hbase"},{"location":"setup/hbase/#build-docker-image-for-hbase","text":"","title":"Build Docker Image for Hbase"},{"location":"setup/hbase/#maven-pom","text":"pom.xml is written to bundle all of the dependencies to package in docker. Such has downloading hadoop and hbase binaries from either public repos or private repos of organisation. Optionally you can customise the package to suit your needs. Some Examples are: optional libraries for change propogation, modified hbase base libraries, repair libraries, etc. Additionally you can have modified repositories , distributionManagement , pluginRepositories so as to download dependencies from private repositories","title":"Maven POM"},{"location":"setup/hbase/#dockerfile","text":"Dockerfile package hbase and hadoop binaries, which can be downloaded from public mirrors or private mirrors. User, Group, Directories required are created and given sufficient permissions for hbase to run. Optionally can modify or add additional libraries from hbase or hadoop packages Optionally utilities can be installed required such as dnsutils, netcat, etc Base image should be kept smaller, builder image can include jdk image Build docker image and publish to a repository. docker build . --network host -t hbase:2.4.2 && docker push hbase:2.4.2","title":"Dockerfile"},{"location":"setup/hbase/#hbase-standalone","text":"","title":"Hbase Standalone"},{"location":"setup/hbase/#package-and-deploy-hbase-standalone","text":"","title":"Package and Deploy Hbase Standalone"},{"location":"setup/hbase/#helm-chart","text":"A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasestandalone-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasestandalone-chart/charts/ Open examples/hbasestandalone-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are image: Docker image of hbase we built in previous section Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasestandalone-chart hbasestandalone-chart/ -n hbase_standalone","title":"Helm Chart"},{"location":"setup/hbase/#hbase-cluster","text":"","title":"Hbase Cluster"},{"location":"setup/hbase/#package-and-deploy-hbase-cluster","text":"","title":"Package and Deploy Hbase Cluster"},{"location":"setup/hbase/#helm-chart_1","text":"Changing namespace names would mean configuration having host names should also be changed such as zookeeper, namenode etc A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasecluster-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasecluster-chart/charts/ Open examples/hbasecluster-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are isBootstrap: Enable this flag first time you run this cluster. Which performs hdfs format , required at the time of cluster setup. Once cluster started, you can disable and upgrade the cluster again. image: Docker image of hbase we built in previous section annotations: In this examples, we have used to demonstrate MTL (Monitoring, Telemetry and Logging) Volume claims for your k8s can be fetched using kubectl get storageclass . Which can be used to replace storageClass probeDelay : This will affect both liveness and readiness alike Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasecluster-chart hbasecluster-chart/ -n hbase_cluster","title":"Helm Chart"},{"location":"setup/hbase/#hbase-tenant","text":"","title":"Hbase Tenant"},{"location":"setup/hbase/#operator-side","text":"Add additional namespaces to watch for. Here specific namespace to be onboarded for a particular tenant vim operator/config/custom/config/hbase-operator-config.yaml Create configmap with command kubectl apply -f operator/config/custom/config/hbase-operator-config.yaml -n hbase_operator Deploy operator","title":"Operator Side"},{"location":"setup/hbase/#tenant-side","text":"Create Rolebinding under namespace which is hosting either hbasetenant or hbasecluster such as follows. Where hbase_tenant is the namespace on which you would deploy your resources ./testbin/bin/kubectl apply -f config/rbac/role_binding.yaml -n hbase_tenant","title":"Tenant Side"},{"location":"setup/hbase/#package-and-deploy-hbase-tenant","text":"","title":"Package and Deploy Hbase Tenant"},{"location":"setup/hbase/#helm-chart_2","text":"Changing namespace names would mean configuration having host names should also be changed such as zookeeper, namenode etc A customisable base helm chart is available to make use of and simplify deployable helm charts. You can find ./helm-charts/hbase-chart/ under root folder of this repository Build the base helm chart from root folder of this repository as follows helm package helm-charts/hbase-chart/ You can find package hbase-chart-x.x.x.tgz created under root folder of this repository. Otherwise you can publish chart to jfrog or harbor or any other chart registry. For manual testing, you can move hbase-chart-x.x.x.tgz under examples/hbasetenant-chart/charts/ cd hbase-operator && mv hbase-chart-x.x.x.tgz examples/hbasetenant-chart/charts/ Open examples/hbasetenant-chart/values.yaml , and modify the values as per your requirement. Some of the recommended modifications are image: Docker image of hbase we built in previous section annotations: In this examples, we have used to demonstrate MTL (Monitoring, Telemetry and Logging) Volume claims for your k8s can be fetched using kubectl get storageclass . Which can be used to replace storageClass probeDelay : This will affect both liveness and readiness alike Memory limits / requests and CPU limits / request as per your requirements You can deploy your helm package using following command helm upgrade --install --debug hbasetenant-chart hbasetenant-chart/ -n hbase_tenant","title":"Helm Chart"},{"location":"setup/operator/","text":"Deploy Operator \u00b6 Hbase operator is written to understand hbase tenant and hbase cluster Custom Resource Definitions of kubernetes. Build Docker image for Hbase Operator \u00b6 navigate to /operator under parent directory of this repository You can build operator using following command make docker-build IMG=hbase-operator:v1.0.0 or docker build -f Dockerfile -t hbase-operator:v1.0.0 . Push docker image to remote registry docker push hbase-operator:v1.0.0 Deploy Operator \u00b6 Via Makefile \u00b6 navigate to /operator under parent directory of this repository Deploy operator image in your kubernetes. Use -n optionally to specify namespace make deploy IMG=hbase-operator:v1.0.0 -n hbase-operator-ns UnDeploy operator. Use -n optionally to specify namespace make undeploy IMG=hbase-operator:v1.0.0 -n hbase-operator-ns Hbase operator is verbose enough on any operations performed. You can check container logs using kubectl or other mechanism. Example kubectl logs hbase-operator-controller-manager-76b4455b76-t4bbb -c manager -f -n hbase-operator-ns Via Helm Chart \u00b6 Base Helm Chart: You can find base helm chart which packages all the necessary manifests into single package. Navigate to helm-charts/operator-chart from root directory of this repository. You can build the package using following command helm package helm-charts/operator-chart/ Deploy Helm Chart: : You can find example helm chart to deploy operator under examples/operator-chart helm upgrade --install --debug example examples/operator-chart/ -n hbase-operator-ns Hbase operator is verbose enough on any operations performed. You can check container logs using kubectl or other mechanism. Example kubectl logs hbase-operator-controller-manager-76b4455b76-t4bbb -c manager -f -n hbase-operator-ns","title":"Deploy Operator"},{"location":"setup/operator/#deploy-operator","text":"Hbase operator is written to understand hbase tenant and hbase cluster Custom Resource Definitions of kubernetes.","title":"Deploy Operator"},{"location":"setup/operator/#build-docker-image-for-hbase-operator","text":"navigate to /operator under parent directory of this repository You can build operator using following command make docker-build IMG=hbase-operator:v1.0.0 or docker build -f Dockerfile -t hbase-operator:v1.0.0 . Push docker image to remote registry docker push hbase-operator:v1.0.0","title":"Build Docker image for Hbase Operator"},{"location":"setup/operator/#deploy-operator_1","text":"","title":"Deploy Operator"},{"location":"setup/operator/#via-makefile","text":"navigate to /operator under parent directory of this repository Deploy operator image in your kubernetes. Use -n optionally to specify namespace make deploy IMG=hbase-operator:v1.0.0 -n hbase-operator-ns UnDeploy operator. Use -n optionally to specify namespace make undeploy IMG=hbase-operator:v1.0.0 -n hbase-operator-ns Hbase operator is verbose enough on any operations performed. You can check container logs using kubectl or other mechanism. Example kubectl logs hbase-operator-controller-manager-76b4455b76-t4bbb -c manager -f -n hbase-operator-ns","title":"Via Makefile"},{"location":"setup/operator/#via-helm-chart","text":"Base Helm Chart: You can find base helm chart which packages all the necessary manifests into single package. Navigate to helm-charts/operator-chart from root directory of this repository. You can build the package using following command helm package helm-charts/operator-chart/ Deploy Helm Chart: : You can find example helm chart to deploy operator under examples/operator-chart helm upgrade --install --debug example examples/operator-chart/ -n hbase-operator-ns Hbase operator is verbose enough on any operations performed. You can check container logs using kubectl or other mechanism. Example kubectl logs hbase-operator-controller-manager-76b4455b76-t4bbb -c manager -f -n hbase-operator-ns","title":"Via Helm Chart"},{"location":"setup/resources/","text":"change directory to /operator under parent directory of this repository Create required resource \u00b6 Extract crds and apply it on your k8s cluster Apply crds on your cluster using kubectl ./bin/kustomize build config/crd | kubectl apply -f - Or generate the crd as follows and apply using some automation tool ./bin/kustomize build config/crd Create namespaces if not already created. lets keep hbase-operator-ns for namespace on which operator will be deployed, hbase-cluster-ns for namespace on which hbase cluster will be deployed and hbase-tenant-ns for namespace on which tenant will be deployed. kubectl create namespace hbase-operator-ns kubectl create namespace hbase-cluster-ns kubectl create namespace hbase-tenant-ns kubectl create namespace hbase-standalone-ns RBAC for multi namespace deployment (Operator is deployed in its own namespace different from either cluster or tenant namesapces) Create ClusterRole with permissions required for operator to apply on namespaces. Assuming operator is on different namespace from hbasecluster and or tenant. Modify Role to ClusterRole in config/rbac/role.yaml in case you want to have global scope or else apply hbase-cluster-ns or hbase-tenant-ns namespace without any changes kubectl apply -f config/rbac/role.yaml Or Apply contents from config/rbac/role.yaml using some automation tool Create RoleBilding under namespace which is hosting either hbase-tenant-ns or hbase-cluster-ns such as follows. Where hbase-tenant-ns and hbase-cluster-ns are the namespace on which you would deploy your resources kubectl apply -f config/rbac/role_binding.yaml -n hbase-cluster-ns kubectl apply -f config/rbac/role_binding.yaml -n hbase-tenant-ns Service Account and roleRef particulars should match with which operator will be run along with namespace RBAC for single namespace deployment (Operator is deployed along with hbase cluster/tenant in single namespace) Create Role with permissions required for operator to apply on namespaces. ./testbin/bin/kubectl apply -f config/rbac/role.yaml Create RoleBilding under same namespace. ./testbin/bin/kubectl apply -f config/rbac/role_binding.yaml Service Account and roleRef particulars should match with which operator will be run along with namespace","title":"First Time Setup"},{"location":"setup/resources/#create-required-resource","text":"Extract crds and apply it on your k8s cluster Apply crds on your cluster using kubectl ./bin/kustomize build config/crd | kubectl apply -f - Or generate the crd as follows and apply using some automation tool ./bin/kustomize build config/crd Create namespaces if not already created. lets keep hbase-operator-ns for namespace on which operator will be deployed, hbase-cluster-ns for namespace on which hbase cluster will be deployed and hbase-tenant-ns for namespace on which tenant will be deployed. kubectl create namespace hbase-operator-ns kubectl create namespace hbase-cluster-ns kubectl create namespace hbase-tenant-ns kubectl create namespace hbase-standalone-ns RBAC for multi namespace deployment (Operator is deployed in its own namespace different from either cluster or tenant namesapces) Create ClusterRole with permissions required for operator to apply on namespaces. Assuming operator is on different namespace from hbasecluster and or tenant. Modify Role to ClusterRole in config/rbac/role.yaml in case you want to have global scope or else apply hbase-cluster-ns or hbase-tenant-ns namespace without any changes kubectl apply -f config/rbac/role.yaml Or Apply contents from config/rbac/role.yaml using some automation tool Create RoleBilding under namespace which is hosting either hbase-tenant-ns or hbase-cluster-ns such as follows. Where hbase-tenant-ns and hbase-cluster-ns are the namespace on which you would deploy your resources kubectl apply -f config/rbac/role_binding.yaml -n hbase-cluster-ns kubectl apply -f config/rbac/role_binding.yaml -n hbase-tenant-ns Service Account and roleRef particulars should match with which operator will be run along with namespace RBAC for single namespace deployment (Operator is deployed along with hbase cluster/tenant in single namespace) Create Role with permissions required for operator to apply on namespaces. ./testbin/bin/kubectl apply -f config/rbac/role.yaml Create RoleBilding under same namespace. ./testbin/bin/kubectl apply -f config/rbac/role_binding.yaml Service Account and roleRef particulars should match with which operator will be run along with namespace","title":"Create required resource"},{"location":"setup/additional/rackawareness/","text":"Rackawareness \u00b6 How to enable \u00b6 Build rack utils docker image as below docker build utilities/rackuitls/ --network host --build-arg AppName=\"RackUtils\" -t \"hbase-rack-utils:1.0.0\" docker push hbase-rack-utils:1.0.0 Enable sidecar along with hmaster container as described below in values.yaml file sidecarcontainers: - name: rackutils image: hbase-rack-utils:1.0.0 cpuLimit: 0.2 memoryLimit: 256Mi cpuRequest: 0.2 memoryRequest: 256Mi runAsUser: 1011 runAsGroup: 1011 command: [\"./entrypoint\"] args: [\"com.flipkart.hbase.HbaseRackUtils\", \"/etc/hbase\", \"/hbase-operator\", \"/opt/share/rack_topology.data\"] volumeMounts: - name: data mountPath: /opt/share /etc/hbase is where hbase configuration is mounted /hbase-operator is zookeeper znode where rack topology information is stored for each datanode /opt/share/rack_topology.data is path on hmaster container where topology information is stored Command using which rack(fault domain) information can be fetched from each datanode commands: faultDomainCommand: \"cat /etc/nodeinfo | grep 'smd' | sed 's/smd=//' | sed 's/\\\"//g'\" Add following configuration in hbase-site.xml <property> <name>net.topology.script.file.name</name> <value>/opt/scripts/rack_topology</value> </property> How it works \u00b6 Refer to previous section before reading further Rack state for each datanode is stored in zookeeper znode example: /hbase-operator Each datanode has init container refer: chart/templates/meta/_faultdomain.tpl which at the time of creation updates its latest faultdomain in znode Hmaster side container as described above, reads the znode /hbase-operator and constructs topology file /opt/share/rack_topology.data Topology file is configured in hbase-site.xml for hmaster to read each time a region assignment is made. Favored nodes of each region would be on different racks wherever possible.","title":"RackAwareness"},{"location":"setup/additional/rackawareness/#rackawareness","text":"","title":"Rackawareness"},{"location":"setup/additional/rackawareness/#how-to-enable","text":"Build rack utils docker image as below docker build utilities/rackuitls/ --network host --build-arg AppName=\"RackUtils\" -t \"hbase-rack-utils:1.0.0\" docker push hbase-rack-utils:1.0.0 Enable sidecar along with hmaster container as described below in values.yaml file sidecarcontainers: - name: rackutils image: hbase-rack-utils:1.0.0 cpuLimit: 0.2 memoryLimit: 256Mi cpuRequest: 0.2 memoryRequest: 256Mi runAsUser: 1011 runAsGroup: 1011 command: [\"./entrypoint\"] args: [\"com.flipkart.hbase.HbaseRackUtils\", \"/etc/hbase\", \"/hbase-operator\", \"/opt/share/rack_topology.data\"] volumeMounts: - name: data mountPath: /opt/share /etc/hbase is where hbase configuration is mounted /hbase-operator is zookeeper znode where rack topology information is stored for each datanode /opt/share/rack_topology.data is path on hmaster container where topology information is stored Command using which rack(fault domain) information can be fetched from each datanode commands: faultDomainCommand: \"cat /etc/nodeinfo | grep 'smd' | sed 's/smd=//' | sed 's/\\\"//g'\" Add following configuration in hbase-site.xml <property> <name>net.topology.script.file.name</name> <value>/opt/scripts/rack_topology</value> </property>","title":"How to enable"},{"location":"setup/additional/rackawareness/#how-it-works","text":"Refer to previous section before reading further Rack state for each datanode is stored in zookeeper znode example: /hbase-operator Each datanode has init container refer: chart/templates/meta/_faultdomain.tpl which at the time of creation updates its latest faultdomain in znode Hmaster side container as described above, reads the znode /hbase-operator and constructs topology file /opt/share/rack_topology.data Topology file is configured in hbase-site.xml for hmaster to read each time a region assignment is made. Favored nodes of each region would be on different racks wherever possible.","title":"How it works"}]}